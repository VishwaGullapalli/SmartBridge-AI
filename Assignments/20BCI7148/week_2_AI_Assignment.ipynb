{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tUe6NlgAvTrv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Step 1: Read the dataset\n",
        "df = pd.read_csv('/content/drug200.csv')\n",
        "\n",
        "# Step 2: Perform data pre-processing\n",
        "# Separate features (X) and labels (y)\n",
        "X = df.drop(['Drug'], axis=1)\n",
        "y = df['Drug']\n",
        "\n",
        "# Perform label encoding on the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Perform one-hot encoding on categorical features (if any)\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Perform feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Step 1: Build the ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Step 2: Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2icUSEakvVKf",
        "outputId": "6716285c-b9f0-4a1f-d8ec-70f391083e66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 1s 5ms/step - loss: 1.5331 - accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.3593 - accuracy: 0.5875\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.2252 - accuracy: 0.6500\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.0949 - accuracy: 0.6562\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.9780 - accuracy: 0.6562\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.8649 - accuracy: 0.7000\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.7514 - accuracy: 0.7437\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6463 - accuracy: 0.7937\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.8687\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.9062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f278864b490>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate random data for testing\n",
        "random_data = np.random.rand(5, X_train_scaled.shape[1])  # Replace 5 with the desired number of samples\n",
        "\n",
        "# Scale the random data\n",
        "random_data_scaled = scaler.transform(random_data)\n",
        "\n",
        "# Predict the drug classes for the random data\n",
        "predictions = model.predict(random_data_scaled)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "predicted_drugs = label_encoder.inverse_transform(predicted_classes)\n",
        "\n",
        "# Print the predicted drug classes\n",
        "print(\"Predicted Drug Classes:\")\n",
        "for drug in predicted_drugs:\n",
        "    print(drug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnLfMktrvZFs",
        "outputId": "a8c7376a-adc6-47a9-ce41-356df0db66c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 141ms/step\n",
            "Predicted Drug Classes:\n",
            "drugX\n",
            "drugX\n",
            "drugX\n",
            "drugX\n",
            "drugX\n"
          ]
        }
      ]
    }
  ]
}